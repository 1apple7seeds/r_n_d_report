%!TEX root = ../report.tex

\chapter{State of the Art}

\section{Robot Motion Planning}
  
"...eminently necessary since, by definition, a robot accomplishes tasks by moving in the real world." - J.-C. Latombe (1991). The first requirement of a robot, no-matter whether it is a manipulator, a mobile robotic platform or a drone, is to be able to move in the environment to accomplish its tasks. Robot Motion Panning is a decades old concept and over the time various algorithms have been developed to tackle the problem of planning which are good enough to provide  solutions for real world situations. Few of them are summarized below.\\
Potential field methods for motion planning were proposed in the early period, which follow the gradient of a potential that guides a robot to its goal \cite{khatib1986real}. It is difficult, though, to come up with a general mechanism to escape local minima of a potential function or design a potential function that has only one minimum. \\
Another family of planning algorithms is composed of heuristic search techniques (e.g., A*) that operate over a discretization of possible robot configurations. These algorithms provide resolution completeness: A path will be found if the discretization is fine enough. A careful choice of resolution and heuristics is critical for efficient heuristic search. But complexity of these algorithm makes it hard to scale them for higher dimensions and adopt them for constraints \cite{kingston2018sampling}. 

Dynamic programming is one the algorithm which is proven to be good fo low dimensional problems. It is mathematical optimization method where set of constraints on robot motion is provided and algorithm tries to optimize the trajectory to meet all the constraints. 

Recently, approaches that use penalty functions to optimize the trajectory have been proposed. They soft the hard constraints into soft constraints and combine them into one formulation. By optimizing the penalty function, a optimized trajectory against parameters like time, energy, etc. is obtained. But these algorithms suffer from the problem of being trapped in local minima and may require huge amount of computation leaving them useless for real time application in robots with limited computation power\cite{kingston2018sampling}.
  
Sampling-based algorithms take a very different approach. They randomly sample valid robot configurations and form a graph of valid motions. Many algorithms provide probabilistic completeness: The probability of finding a solution goes to 1 with the run time of the algorithm,
provided a solution exists. Sampling-based motion planning algorithms are effective at solving motion planning problems in a broad range of settings with minimal changes, including very-high-dimensional systems\cite{kingston2018sampling}. 

The Rapidly-exploring Random Tree (RRT) is sampling based exploration algorithm for quickly searching high dimensional spaces that have both global constraints (arising from workspace obstacles and velocity bounds) and differential constraints (arising from kinematics and dynamics) \cite{lavalle2003dynamic}. The key idea is to bias the exploration toward unexplored portions of the space by randomly sampling points in the state space, and incrementally “pulling” the search tree toward them. RRT can be used for motion planning by using randomly generated tree in configuration space and validating it using the constraints defined. Not every solution found by RRT is feasible and optimal which makes method incomplete and non-optimal. But this method is quiet effective in in high dimensional problems with numerous constraints. 

MoveIt!, one of the widely used motion planning and control framework, uses sampling based motion planing algorithms provided by Open Motion Planning Library (OMPL) by default. The solutions generated by sampling based algorithms, which use inverse kinematic solvers to find configuration space samples, are often not continuous. It can be understood by following example. 

Moreover, motion plans are often generated without considering robots ability to execute them in real world, the uncertainties like slip, inaccuracy in executions, and dynamic changes in the environment like real time change in goal or moving obstacle. Often these dynamic changes and uncertainties lead to inaccurate manipulation and re-planning. Theoretically it is possible to accommodate these dynamic environmental parameters into a plan but it hard and complex to do so. 

Another way to go is to generate a control policy instead of generating motion plan. Mechanisms can be developed to accommodate uncertainties and dynamic changes in environment and easily adopted into control policy.  

\section{Control Policy Search in Robotics}

Control policy search algorithms can be categorized into two categories, \textit{model-based policy search} and \textit{model-free policy search}. Model based policy search algorithms need a fairly accurate model of robot as well as the environment. Using the observed trajectories, forward model of the robot's dynamics and environment is learned. This forward model is used for internal simulations for validation of the generated trajectory. (e.g collision, reachability). These methods heavily suffer from inaccurate model. Control policies generated by using inaccurate model are not robust and can be dangerous to execute in real world scenarios.  

In model-free methods, a trajectory generation policy is learned without considering model of the robot. An external framework is used for validation and control of trajectories generated. Learning a policy is often easier than learning accurate forward models. Hence model-free policies are more widely used than model-based methods\cite{deisenroth2013survey}. 

Deisenroth et al.\cite{deisenroth2013survey} presented survey on policy search methods in robotics. Model free policy search methods have obvious advantage over model-based methods as accurate model of robot and environment is not needed which is often very difficult to obtain. Paper also discusses main policy representations with their advantages and disadvantages.  

\textbf{Linear Policies:} Linear controllers are the most simple time independent representation. Policies are represented as linear combination of basis function. However, specifying the basis functions by hand is typically a difficult task, and, hence, the application of linear controllers is limited to problems where appropriate basis functions are known\cite{deisenroth2013survey}.

\textbf{Radial Basis Functions Networks:} A typical nonlinear time independent policy representation is a radial basis function (RBF) network. RBF networks are powerful policy representations, they are also difficult to learn due to the high number of nonlinear parameters. RBF networks are local representations, they are hard to scale to high-dimensional state spaces\cite{deisenroth2013survey}.


\textbf{Dynamic Movement Primitives:} Dynamic Movement Primitives (DMPs) are the most widely used time-dependent policy representation in robotics. 
\newline


\section{Control Policy Representation} 
In the field of Learning from Demonstration (\textit{LfD}) and Reinforcement Learning, methods used for representing underlying control policy has significant impact on generalization ability, stability and computational complexity of the approach. In LfD, control policy is learned from state-action examples whereas in reinforcement learning control policy is learned form general experience of the robot. 

%Reinforcement Learning in Robotics: A Survey

In \cite{kober2013reinforcement}, Kober et.al. presented comprehensive survey on function approximation methods for control policy representation. Much of the success of reinforcement learning methods has been due to the clever use of such approximate representations. Main approaches for policy representation : 


\textbf{Via Points \& Splines :} An open-loop policy may often be naturally represented as a trajectory, either in the space of states or targets or directly as a set of controls. Such spline-based policies are very suitable for compressing complex trajectories into few parameters. Typically the desired joint or Cartesian position, velocities, and/or accelerations are used as actions. To minimize the required number of parameters, not every point is stored. Instead, only important via-points are considered and other points are interpolated.

\textbf{Neural Networks : } Neural networks are another general function approximation used to represent policies. Neural oscillators with sensor feedback have been used to learn rhythmic movements where open and closed-loop information were combined, such as gaits for a two legged robot.

\textbf{Motor Primitives :} Motor primitives combine linear models describing dynamics with parsimonious movement parametrizations. While originally biologically-inspired, they have a lot of success for representing basic movements in robotics such as a reaching movement or basic locomotion. These basic movements can subsequently be sequenced and/or combined to achieve more complex movements. Dynamic Movement Primitives is the most successful example of such control policy. 

\textbf{Gaussian Mixture Models and Radial Basis Function Models :} When more general policies with a strong state-dependence are needed, general function approximators based on radial basis functions, also called Gaussian kernels, become reasonable choices.
This approach has been used to generalize a open-loop reaching movement and to learn the closed-loop cart-pole swing-up task.

\textbf{Non-parametric Policies :} Polices based on non- parametric regression approaches often allow a more data-driven learning process. This approach is often preferable over the purely parametric policies listed above because the policy structure can evolve during the learning process. Such approaches are especially useful when a policy learned to adjust the existing behaviors of an lower-level controller. 


\section{Control Policy Representation Using Movement Primitives}
Movement Primitives (MPs) are commonly used for representing and learning basic movements in robotics. MP formulations are compact parameterizations of the robot’s control policy. Modulating their parameters permits imitation and reinforcement learning as well as adapting to different scenarios \cite{paraschos2013probabilistic}. 

\subsection{Motion Primitives Using Principle Component Analysis}
In \cite{lim2005movement}, authors have proposed a high-level framework for robot movement coordination and learning that combines elements of movement storage, dynamic models, and optimization, with the ultimate objective of generating natural, human-like motions. Movement primitive is represented and stored as a set of joint trajectory basis functions; these basis functions are extracted via a principal component analysis of human motion capture data. Dynamics based optimization is performed on the learned movement primitives to optimize them to use minimum torque. They also discuss about the general framework to deploy the movement primitive in real world task. In this, a task parser uses description of task to generate a sequence of the movements. Then the movement compiler chooses appropriate motion primitives to perform sequence of movements. 
\par This method learns movement primitives in terms of joint trajectories, hence it has limited generalization ability when it comes to task space goal execution. This class of movement primitives does not incorporate feedback from the environment.   

\subsection{Probabilistic Movement Primitives}

Paraschos et al. in \cite{paraschos2013probabilistic} present probabilistic approach to model movement primitives called Probabilistic Movement Primitives (ProMPs). In this framework, a MP describes multiple ways to execute a movement, which naturally leads to a probability distribution over trajectories. These motion primitives are able to generalize the motion by encoding variance of the trajectory from multiple demonstration of same motion. Temporal modulation is possible with additional phase variable. The choice of the basis functions depends on the type of movement, which can be either rhythmic or stroke-based. ProMPs support simultaneous activation, match the quality of the encoded behavior from the demonstrations, are able to adapt to different desired target positions, and efficiently learn by imitation. 



\subsection{Dynamic Movement Primitives}
% Movement imitation with nonlinear dynamical systems in humanoid robots

Ijspeert et. al. in \cite{ijspeert2002movement} used the system of autonomous second order differential equations for encoding control policy. This formulation was motivated by following five goals:
\begin{enumerate}
	\item The ease of representing and learning a desired trajectory, 
	\item Compactness of the representation, 
	\item Robustness against perturbations and changes in a dynamic environment, 
	\item Ease of re-use for related tasks and easy modification for new tasks, and 
	\item Ease of categorization for movement recognition.
\end{enumerate}
Trajectory information is encoded in the second order differential equation of damped mass spring system with a non-linear forcing term which is used for modifying the shape of trajectory. The non-linear forcing term is normalized weighted sum of Gaussians timed by a state vector whose evolution is guaranteed by another linear second order differential system. This whole system is stable and converges in limited time. Weights of the Gaussians are learned by using locally weighted regression. This work was the necessary foundation of the theory of dynamic motion primitives. 
\begin{equation} \label{Ijspeert_1}
	\dot{z} = \alpha_{z}(\beta_{z}(g - y) - z)
\end{equation}
\begin{equation} \label{Ijspeert_2}
	\dot{y} = z + \frac{\sum_{i=1}^{N}\psi_{i}w_{i}}{\sum_{i=1}^{N}\psi_{i}}
\end{equation}
Above system of equations is essentially a simple second-order system with the exception that its velocity is modified by a nonlinear term (the second term in equation \ref{Ijspeert_2}) which depends on internal states. These two internal states, $(v,x)$ have the following second-order linear dynamics
\begin{equation}
\dot{z} = \alpha_{v}(\beta_{v}(g - x) - v)
\end{equation}
\begin{equation}
\dot{x} = v
\end{equation}
and $\psi_{i}$ is given by,
\begin{equation}
\psi = exp(-\frac{1}{2\sigma_{i}^{2}}(\tilde{x} - c_{i}^{2}))
\end{equation}
where $\tilde{x} = (x - x0)/(g - x0)$


Use of this second dynamical system allows modification in the speed of execution, motion can even be halted. 

%Learning Attractor Landscapes for Learning Motor Primitives

Ijspeert et al. in \cite{ijspeert2003learning} presented the idea of learning a complex control policy by transforming
an existing simple canonical control policy instead of learning it from scratch. They chose stable differential equation system as the canonical policy. By nonlinearly transforming the canonical attractor dynamics using techniques from nonparametric regression, almost arbitrary new nonlinear policies can be generated without losing the stability properties of the canonical system. Portions of the work presented in this paper were published in \cite{ijspeert2002movement}. Preliminary work in \cite{ijspeert2002movement} was extended with an improvement and simplification of the rhythmic system, an integrated view of the interpretation of both the discrete and rhythmic CPs, the fitting of a complete alphabet of Grafitti characters, and an implementation of automatic allocation of centers of kernel functions for locally weighted learning. In this work authors explained the properties of temporal and spatial invariance i.e. shape of the trajectories does not depend on goal separation or speed of execution. Also authors argued about the robustness against perturbations which later become the foundation for obstacle avoidance properties and extra coupling terms such as feedback.
Experimental evaluation consisted of successful implementation on of rhythmic and discrete control policies on 30 DOFs hydraulic anthropomorphic robot using learning from demonstration technique. This evaluation also gave the glimpse of possible extension of this approach for movement recognition by observing co-relation between weights of the sample movements (stored as a library) and the one which is to be recognized. This was demonstrated by fitting of a complete alphabet of Grafitti characters with 84\% success rate. 
\newline
Stefan Schaal in \cite{schaal2006dynamic} formulated DMPs and implemented DMP system on a 30 DOF Sarcos Humanoid robot. Main contribution of this paper was to define terminology for DMP framework and standardizing the equations by separating the canonical system. 
New DMP framework is represented by following set of equations,  
\begin{equation}
\tau\dot{z} = \alpha_{z}(\beta_{z}(g - y) - z) + f
\end{equation}
\begin{equation}
\tau \dot{y} = z
\end{equation}
and the non-linear function $f$
\begin{equation}
f(x) = \frac{\sum_{i=1}^{N}\psi_{i}(t)w_{i}}{\sum_{i=1}^{N}\psi_{i}(t)}x(g - y_{0})
\end{equation}
where,
\begin{equation} \label{schaal_1}
\tau \dot{x} = -\alpha_{x}x
\end{equation}
and,
\begin{equation}
\psi_{i} = \exp(-{\frac{1}{2\sigma_{i}^{2}}(x - c_{i})^{2}})
\end{equation}
Eq. \ref{schaal_1} is called canonical system which eliminates the time dependency. It is also used for co-ordination between different degrees of freedom. In typical robotic application, each DMP corresponds to one controlled variable, which might be, for example, one of the joint coordinates or one of the Cartesian coordinates and all DMPs share the same phase variable. In this work, humanoid robot demonstrated drumming task and tennis swing task. Point attractive behavior of dynamical system enabled the task of tennis swing as it is discrete in nature and limit cycle behavior enabled drumming task which is rhythmic or periodic in nature.   
\newline
Pastor et al. in \cite{pastor2009learning} provided a general approach for learning robotic motor skills from human demonstration. They identified important features of DMP framework represented by above equations. They are:
\begin{itemize}
	\item Convergence to the goal g is guaranteed (for bounded weights) since non-linear term $f$ vanishes at the end of a movement.
	\item The weights $w_{i}$ can be learned to generate any desired smooth trajectory.
	\item The equations are spatial and temporal invariant, i.e., movements are self-similar for a change in goal, start point, and temporal scaling without a need to change the weights $w_{i}$.
	\item The formulation generates movements which are robust against perturbation due to the inherent attractor dynamics of the equations.
\end{itemize}
Based on the DMP representation of the movements library of movements was built by labeling each recorded movement according to task and context (e.g., grasping, placing, and releasing). Semantic information stored can be used to choose particular DMP for doing a certain task. Semantic information was added to movement primitives, such that they can code object oriented action. Differential equation used is formulated such that generalization can be achieved simply by adapting a start and a goal parameter in the equation to the desired position values of a movement. For sequencing DMPs, initial conditions of next DMP were modified and it was started before finishing the current one so that there is no jump in the acceleration and velocity. The idea of building library of motion primitives and attaching semantics is coined in this work but such motion movement primitives were handpicked at the time of demonstration. No automated algorithm was presented to choose the combination of movement primitives for achieving complex task. 
\newline
In \cite{lioutikov2016learning}, sequencing of motion primitives was implemented for the task of cutting vegetables. Basic primitives were which are necessary for performing given complex task were learned and then sequenced to perform the task. The experiment was conducted on Darias robot platform which consists of dual-arm setup using two 7 DOF KUKA Light Weight Robot arms. This work shows that sequencing learned motion primitives to do complex task is possible by enforcing goal state of preceding primitive and initial state of next primitive to be same. But this work doesn't specify any method for combining arbitrary motion primitives in order to do entirely new task. 
\newline
Another approach for sequencing of DMPs is presented in \cite{nemec2012action}. A method for achieving continuous acceleration at the moment of transition from one DMP to another DMP is to represent DMPs in the form of $3^{rd}$ order differential equation. This representation will enable us to provide initial velocity along with position and yield differentiable acceleration term. Another way to achieve this is online Gaussian kernel functions modification of the second order dynamic motion primitives. Both methods were tested in simulation using a simple illustrative example, where authors joined two ramp functions, as well as in real-world experiments that included pouring a liquid into a glass, table wiping, and carrying a glass. The third experiment shows that continuous accelerations are essential when performing typical kitchen scenario tasks such as carrying a glass of liquid, even at relatively low velocities. It has been shown that both proposed approaches are appropriate when joining any combination of discrete and rhythmic motions for the cases where smooth accelerations are important.
\newline 
Work presented in \cite{park2008movement} solves problem of reproduction of motion in the presence of obstacles. It is achieved by using potential field centered around the obstacle and adding its gradient to the equation of motion. This work also compares different representations of potential fields for obstacle-link collision avoidance. This work also expand the previous work in order to learn motion primitives in end-effector space. This work presents a modification in DMP framework to eliminate the restriction on separation between start and goal position. In previous formulation, start and goal position has to have a separation between them otherwise DMP could not leave the initial position. If this separation is very small, learned DMP might show unstable behavior (very large trajectories for very small change in goal position). 
\newline
New formulation is as follows:
\begin{equation}
	\tau \dot{v} = K(g - x) - Dv - K(g - x_{0})\theta + Kf(\theta) + \psi(x, v)
\end{equation}
\begin{equation}
	\tau \dot{x} = v 
\end{equation}
\begin{equation}
	\tau \dot{\theta} = \alpha \theta 
\end{equation}
Here forcing term $f(\theta)$ is no longer multiplied by $(g - x_{0})$ term, hence DMP is no longer dependent on initial position and goal separation to take off. The term $\psi(x, v)$ is extra coupling term or perturbation term added to deal with the obstacle which is essentially the gradient of static or dynamic potential field around the obstacle which will repel the trajectory. 
\newline
Static potential field is given by:
\begin{equation}
	U_{static} = \frac{\eta}{2}(\frac{1}{p(x)} - \frac{1}{p_{0}})^{2}
\end{equation}

where $p_{0}$ is the radius of influence of the obstacle, $p(x)$ is distance of end effector from obstacle, and $\eta$ is a constant gain. This field is zero outside the radius $p_{0}$.
\newline
Dynamic potential field is given by:
\begin{equation}
	U_dynamic = \lambda(-cos(\theta))^{\beta}\frac{||v||}{p(x)} \hspace{1cm} : \frac{\pi}{2} < \theta <= \pi
\end{equation}
here, $v$ is relative velocity of obstacle and $\theta$ is angle between relative velocity vector and position vector of obstacle with respect to point of interest (end effector or any point on links). 

Both the potential fields were experimentally tested in simulation as well as on real robot.  
The conventional static potential method shows an unstable avoidance movement, which oscillates. On the other hand, the dynamic potential method results in a smooth obstacle-avoidance movement.
A approach of constraining the null space of the robot to avoid the collision with links of the robot was also tested with dynamic field. Two scenarios were evaluated : a) end-effector position ($x$) as input into the potential field, b) Closest point on manipulator as ($x$) input. In scenario a), large number of trajectories converged to goal than b). But scenario b) was better in terms of robustly avoiding collisions.  
\newline
Meier et.al. presented the probabilistic representation of dynamic motion primitives in \cite{meier2016probabilistic}. In this work, authors showed how DMPs can be reformulated as a probabilistic linear dynamical system with control inputs. Through this probabilistic representation of DMPs, algorithms such as Kalman filtering and smoothing are directly applicable to perform inference on proprioceptive sensor measurements during execution. Inference on this probabilistic form of the DMP allows us to measure the likelihood of DMP being successfully executed. 
\newline
Karlsson et.al.\cite{karlsson2017autonomous} presented the approach for modification of dynamic motion primitives. A modified DMP is formed, based on the first part of the faulty trajectory and the last part of the corrective one. Demonstrations were autonomously interpreted for their quality when a new corrective demonstration was presented to the robot. 
\newline
Hoffman et.al. in \cite{hoffmann2009biologically} presented solution for the problem of automatic real-time goal adaptation and obstacle avoidance in dynamic motion primitives.They also modified the DMP framework to enable use of DMP where there is no offset in initial and goal position. This was not possible in previous formulation.


\subsection{Advantages of Dynamic Movement Primitives}
\begin{itemize}
	\item It is a model free learning approach.
	\item Any arbitrary trajectory can be learned in end-effector space as well as in joint space.
	\item Here learning is linear regression, so it does not need large dataset.
	One trajectory is sufficient ideally.
	\item Trajectories can be scaled in space as well as in time.
	\item Dynamic motion primitives can be initialized anywhere in the attractor space.
	\item Trajectory evolves as robot actually moves along the trajectory. Hence on-line modifications in the trajectory are possible.
	\item These modifications can be realized by introducing vanishing coupling terms in the differential equations (e.g. potential field around a obstacles \cite{park2008movement}). Perturbations can be handled robustly due to this reason.
	\item Re-planning is not needed unless an event causing major disturbance in the environment occurs.
\end{itemize}